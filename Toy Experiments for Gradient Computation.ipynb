{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Does backpropping through a flatten work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "\n",
    "z = x.reshape((2, 2))\n",
    "\n",
    "loss = torch.norm(z) * torch.norm(z)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6., 8.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Computing Hessian-vector products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(324.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(324.)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup for now:\n",
    "# f maps [n], [m] to [1], so Hessian is n x n or n x m or m x m depending on what's being done\n",
    "# Have a vector v, want to compute Hv\n",
    "\n",
    "def f(x, y):\n",
    "    return torch.sum(torch.pow(x, 3)) * torch.sum(torch.pow(y, 3))\n",
    "\n",
    "def g(x, y, theta):\n",
    "    return theta**2 * torch.sum(torch.pow(x, 3)) * torch.sum(torch.pow(y, 3))\n",
    "\n",
    "n = 4\n",
    "m = 3\n",
    "x = torch.Tensor(range(n))\n",
    "y = torch.Tensor(range(m))\n",
    "print(f(x, y))\n",
    "g(x, y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hvp(g, x, y, v):\n",
    "    xvar = Variable(x, requires_grad=True)\n",
    "    yvar = Variable(y, requires_grad=True)\n",
    "    vvar = Variable(v, requires_grad=True)\n",
    "    \n",
    "    score = g(xvar, yvar)\n",
    "    \n",
    "    grad, = torch.autograd.grad(score, yvar, create_graph=True)\n",
    "    #print(grad)\n",
    "    total = torch.sum(grad * vvar)\n",
    "    #print(total)\n",
    "    \n",
    "    if xvar.grad:\n",
    "        xvar.grad.data.zero_()\n",
    "    if yvar.grad:\n",
    "        yvar.grad.data.zero_()\n",
    "        \n",
    "    total.backward()\n",
    "    \n",
    "    if xvar.grad is not None:\n",
    "        return xvar.grad\n",
    "    else:\n",
    "        return torch.zeros(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will allow us to backpropagate hessian-vector products, the create_graph=True is important\n",
    "\n",
    "def hvp2(g, x, y, v):\n",
    "    xvar = Variable(x, requires_grad=True)\n",
    "    yvar = Variable(y, requires_grad=True)\n",
    "    vvar = Variable(v, requires_grad=True)\n",
    "    \n",
    "    score = g(xvar, yvar)\n",
    "    \n",
    "    grad, = torch.autograd.grad(score, yvar, create_graph=True)\n",
    "    #print(grad)\n",
    "    total = torch.sum(grad * vvar)\n",
    "    #print(total)\n",
    "    \n",
    "    if xvar.grad:\n",
    "        xvar.grad.data.zero_()\n",
    "    if yvar.grad:\n",
    "        yvar.grad.data.zero_()\n",
    "        \n",
    "    grad2, = torch.autograd.grad(total, xvar, create_graph=True, allow_unused=True)\n",
    "    return grad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0., 108., 432.], grad_fn=<MulBackward0>)\n",
      "tensor(108., grad_fn=<SumBackward0>)\n",
      "result tensor([ 0.,  9., 36., 81.])\n",
      "tensor([  0., 108., 432.], grad_fn=<MulBackward0>)\n",
      "tensor(108., grad_fn=<SumBackward0>)\n",
      "result tensor([ 0.,  9., 36., 81.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "v = torch.Tensor([2, 1, 0])\n",
    "print(\"result\", hvp(f, x, y, v))\n",
    "print(\"result\", hvp2(f, x, y, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so this breaks when the Hessian doesn't survive somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  9., 36., 81.])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-56006646cf78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# this will not work\n",
    "\n",
    "theta.grad.data.zero_()\n",
    "theta = Variable(torch.Tensor([1]), requires_grad=True)\n",
    "x = torch.Tensor(range(n))\n",
    "y = torch.Tensor(range(m))\n",
    "v = torch.Tensor([2, 1, 0])\n",
    "\n",
    "score = hvp(lambda x, y: g(x, y, theta), x, y, v)\n",
    "print(score)\n",
    "torch.sum(score).backward()\n",
    "print(theta.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  9., 36., 81.], grad_fn=<MulBackward0>)\n",
      "tensor([252.])\n"
     ]
    }
   ],
   "source": [
    "# but this will\n",
    "\n",
    "theta.grad.data.zero_()\n",
    "theta = Variable(torch.Tensor([1]), requires_grad=True)\n",
    "x = torch.Tensor(range(n))\n",
    "y = torch.Tensor(range(m))\n",
    "v = torch.Tensor([2, 1, 0])\n",
    "\n",
    "score = hvp2(lambda x, y: g(x, y, theta), x, y, v)\n",
    "print(score)\n",
    "torch.sum(score).backward()\n",
    "print(theta.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
