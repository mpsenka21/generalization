{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Does backpropping through a flatten work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "\n",
    "z = x.reshape((2, 2))\n",
    "\n",
    "loss = torch.norm(z) * torch.norm(z)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([2., 4., 6., 8.])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], grad_fn=<ViewBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Computing Hessian-vector products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(1296.)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(84.)"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "# setup for now:\n",
    "# f maps [n], [m] to [1], so Hessian is n x n or n x m or m x m depending on what's being done\n",
    "# Have a vector v, want to compute Hv\n",
    "\n",
    "def f(x, y):\n",
    "    return torch.sum(torch.pow(x, 3)) * torch.sum(torch.pow(y, 3))\n",
    "\n",
    "def g(x, y, theta):\n",
    "    # return theta**2 * torch.sum(torch.pow(x,3)) * torch.sum(torch.pow(y, 3))\n",
    "    return x.sum() * y.sum()\n",
    "\n",
    "n = 4\n",
    "m = 3\n",
    "x = torch.Tensor(range(n))\n",
    "y = torch.Tensor(range(m))\n",
    "print(f(x, y))\n",
    "g(x, y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hvp(g, x, y, v):\n",
    "    xvar = Variable(x, requires_grad=True)\n",
    "    yvar = Variable(y, requires_grad=True)\n",
    "    vvar = Variable(v, requires_grad=True)\n",
    "    \n",
    "    score = g(xvar, yvar)\n",
    "    \n",
    "    grad, = torch.autograd.grad(score, yvar, create_graph=True)\n",
    "    #print(grad)\n",
    "    total = torch.sum(grad * vvar)\n",
    "    #print(total)\n",
    "    \n",
    "    if xvar.grad:\n",
    "        xvar.grad.data.zero_()\n",
    "    if yvar.grad:\n",
    "        yvar.grad.data.zero_()\n",
    "        \n",
    "    total.backward()\n",
    "    \n",
    "    if xvar.grad is not None:\n",
    "        return xvar.grad\n",
    "    else:\n",
    "        return torch.zeros(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will allow us to backpropagate hessian-vector products, the create_graph=True is important\n",
    "\n",
    "# g is function to take hessian w.r.t.\n",
    "# x and y are the input variables for g\n",
    "# x1 and x2 are strings, either 'x' or 'y'\n",
    "# --- these indicate which variables to take derivatives w.r.t.\n",
    "# v is vector to get Hessian's action on\n",
    "def hvp2(g, x, y, x1, x2, v):\n",
    "    xvar = Variable(x, requires_grad=True)\n",
    "    yvar = Variable(y, requires_grad=True)\n",
    "    vvar = Variable(v, requires_grad=True)\n",
    "\n",
    "    # choose which variable x1var corresponds to\n",
    "    x1var = xvar if x1=='x' else yvar\n",
    "    x2var = xvar if x2=='x' else yvar\n",
    "    \n",
    "    score = g(xvar, yvar)\n",
    "    \n",
    "    grad, = torch.autograd.grad(score, x1var, create_graph=True)\n",
    "    # print(grad)\n",
    "    total = torch.sum(grad * vvar)\n",
    "    # print(total)\n",
    "    \n",
    "    if xvar.grad:\n",
    "        xvar.grad.data.zero_()\n",
    "    if yvar.grad:\n",
    "        yvar.grad.data.zero_()\n",
    "        \n",
    "    grad2, = torch.autograd.grad(total, x2var, create_graph=True, allow_unused=True)\n",
    "    # print(grad2)\n",
    "    return grad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "result tensor([ 0.,  9., 36., 81.])\nresult tensor([ 0.,  9., 36., 81.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "v = torch.Tensor([2, 1, 0])\n",
    "print(\"result\", hvp(f, x, y, v))\n",
    "print(\"result\", hvp2(f, x, y, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so this breaks when the Hessian doesn't survive somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([ 0.,  9., 36., 81.])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-2cc9cb20aabf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhvp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# this will not work\n",
    "\n",
    "# (First time run): comment out following line:\n",
    "theta.grad.data.zero_()\n",
    "theta = Variable(torch.Tensor([1]), requires_grad=True)\n",
    "x = torch.Tensor(range(n))\n",
    "y = torch.Tensor(range(m))\n",
    "v = torch.Tensor([2, 1, 0])\n",
    "\n",
    "score = hvp(lambda x, y: g(x, y, theta), x, y, v)\n",
    "print(score)\n",
    "torch.sum(score).backward()\n",
    "print(theta.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([ 0.,  9., 36., 81.], grad_fn=<MulBackward0>)\ntensor([252.])\n"
     ]
    }
   ],
   "source": [
    "# but this will\n",
    "\n",
    "# theta.grad.data.zero_()\n",
    "theta = Variable(torch.Tensor([1]), requires_grad=True)\n",
    "x = torch.Tensor(range(n))\n",
    "y = torch.Tensor(range(m))\n",
    "v = torch.Tensor([2, 1, 0])\n",
    "\n",
    "score = hvp2(lambda x, y: g(x, y, theta), x, y, v)\n",
    "print(score)\n",
    "torch.sum(score).backward()\n",
    "print(theta.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([-4., -4., -4., -4.], grad_fn=<ExpandBackward>)\nNone\n"
     ]
    }
   ],
   "source": [
    "# \\nabla_{xx} g\n",
    "\n",
    "# theta.grad.data.zero_()\n",
    "theta = Variable(torch.Tensor([1]), requires_grad=True)\n",
    "x = torch.Tensor(range(n))\n",
    "y = torch.Tensor(range(m))\n",
    "v = torch.Tensor([2, 1, 0, -1])\n",
    "\n",
    "score = hvp2(lambda x, y: g(x, y, theta), x, y, 'x', 'y', v)\n",
    "print(score)\n",
    "torch.sum(score).backward()\n",
    "print(theta.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "f2267886810f2f15ec828da6309447cc90fcc527593918026a87c890b0378843"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}